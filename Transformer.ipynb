{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvAKPOxbBLKw"
      },
      "source": [
        "**Transformer from Scratch**\n",
        "\n",
        "\n",
        "---\n",
        "Transformer to translate English to one of Indian languages - Hindi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxjUr345JCNj"
      },
      "outputs": [],
      "source": [
        "# Install datasets\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC383Gz_I6zP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import tensorflow as tf\n",
        "\n",
        "import math\n",
        "import pickle\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dropout, Dense, Embedding, Layer, LayerNormalization\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvV66b7fNZw5"
      },
      "outputs": [],
      "source": [
        "# Download English-Hindi dataset\n",
        "# Source: https://www.cfilt.iitb.ac.in/iitb_parallel/\n",
        "\n",
        "dataset = load_dataset('cfilt/iitb-english-hindi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTTQUXbAortX",
        "outputId": "73bc7f61-516b-4e61-cc08-3a7c3c461941"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 520\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 2507\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1659083\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThI_yd2_R87E",
        "outputId": "f08f5588-8364-48c9-857e-86e6c47ce354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A random observation...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'translation': {'en': 'Choose file or directory to remove:',\n",
              "  'hi': 'फ़ाइल या निर्देशिका को हटाने के लिए चुनेंः'}}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"A random observation...\\n\")\n",
        "dataset['train'][13000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwHd5MGFRCpM",
        "outputId": "5bb97554-d02f-493c-8bda-4e8a6360950f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 10000\n",
            "Train dataset size: 520\n",
            "Train dataset size: 2507\n"
          ]
        }
      ],
      "source": [
        "# Prepare dataset\n",
        "train_dataset = dataset['train'][:10000]['translation']\n",
        "validation_dataset = dataset['validation']['translation']\n",
        "test_dataset = dataset['test']['translation']\n",
        "\n",
        "print('Train dataset size: {}'.format(len(train_dataset)))\n",
        "print('Train dataset size: {}'.format(len(validation_dataset)))\n",
        "print('Train dataset size: {}'.format(len(test_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCXV3S6_eoXr",
        "outputId": "e6dc1075-4522-47d6-df19-4339418549b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Give your application an accessibility workout',\n",
              " 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_dataset = np.array([])  # Source Language: English\n",
        "tgt_dataset = np.array([])  # Target Languange: Hindi\n",
        "\n",
        "for pair in train_dataset:\n",
        "    src_dataset = np.append(src_dataset, pair['en'])\n",
        "    tgt_dataset = np.append(tgt_dataset, pair['hi'])\n",
        "\n",
        "src_dataset[0], tgt_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hdSPzs9_CEQR",
        "outputId": "7b41c24f-8c96-41e0-a2ab-8f4b0677d752"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'give your application an accessibility workout'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the source (English) text to lowercase\n",
        "src_dataset = np.char.lower(src_dataset)\n",
        "src_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mwHiFSTIYe-"
      },
      "outputs": [],
      "source": [
        "# Remove special characters in both source dataset and target dataset\n",
        "def clean_text(text):\n",
        "  # Retain punctuations ^ a-z0-9.?!,:/- so as to not alter/loose the meaning of text\n",
        "  # Add a whitespace before and after the punctuations\n",
        "  text = re.sub('([.?!,:/-])', r' \\1 ', text)\n",
        "\n",
        "  # Remove additional whitespace\n",
        "  text = text.strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "# Source dataset (English)\n",
        "for idx in range(len(src_dataset)):\n",
        "  src_dataset[idx] = clean_text(src_dataset[idx])\n",
        "\n",
        "# Target dataset (Hindi)\n",
        "for idx in range(len(tgt_dataset)):\n",
        "  tgt_dataset[idx] = clean_text(tgt_dataset[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDll9Ch0RCpO",
        "outputId": "9c4ecb39-2740-40cd-d14a-aa0bf22a25ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('the color and opacity of the highlight border .',\n",
              " 'हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता।')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_dataset[8], tgt_dataset[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "e0LVHdW4iPe3",
        "outputId": "f46b60a8-d558-4e6f-ad3d-dad190fc5340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of sequence in English (source): 41\n",
            "Maximum length of sequence in Hindi (target): 40\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the source and target text\n",
        "def tokenize_text(_dataset):\n",
        "  tknzr = Tokenizer()\n",
        "  tknzr.fit_on_texts(_dataset)\n",
        "  _tokenized = tknzr.texts_to_sequences(_dataset)\n",
        "  _word_to_idx = tknzr.word_index\n",
        "  return tknzr, _tokenized, _word_to_idx\n",
        "\n",
        "src_tknzr, src_sequences, src_vocabulary = tokenize_text(src_dataset)\n",
        "src_max_sequence_length = max(len(sequence) for sequence in src_sequences)\n",
        "print(\"Maximum length of sequence in English (source): %s\" % src_max_sequence_length)\n",
        "\n",
        "tgt_tknzr, tgt_sequences, tgt_vocabulary = tokenize_text(tgt_dataset)\n",
        "tgt_max_sequence_length = max(len(sequence) for sequence in tgt_sequences)\n",
        "print(\"Maximum length of sequence in Hindi (target): %s\" % tgt_max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F8oV8X9Bq2U",
        "outputId": "bf69d3b5-9a16-4549-951c-2845bf52e043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique words in English (source) language: 1558\n",
            "Number of unique words in Hindi (target) language: 2037\n"
          ]
        }
      ],
      "source": [
        "src_vocabulary_size = len(src_vocabulary)\n",
        "print(\"Number of unique words in English (source) language: %s\" % src_vocabulary_size)\n",
        "\n",
        "tgt_vocabulary_size = len(tgt_vocabulary)\n",
        "print(\"Number of unique words in Hindi (target) language: %s\" % tgt_vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1-Kv0KhRCpP",
        "outputId": "3b17d68f-05b8-4a6a-c5e3-1394d1402053"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1, 67, 45, 314, 2, 1, 57, 115], [71, 94, 100, 207, 1, 69, 50, 650])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_sequences[10], tgt_sequences[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pApOOkH0RCpQ"
      },
      "outputs": [],
      "source": [
        "# Prepend and append the sequences with [START] and [END], respectively\n",
        "# [START]: vocabulary_size + 1\n",
        "# [END]: vocabulary_size + 2\n",
        "def add_start_end_tokens(sequence, vocabulary_size):\n",
        "    sequence = [vocabulary_size+1] + sequence + [vocabulary_size+2]\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELQ-OwQPRCpQ"
      },
      "outputs": [],
      "source": [
        "# Source sequences\n",
        "for s_idx in range(len(src_sequences)):\n",
        "  src_sequences[s_idx] = add_start_end_tokens(src_sequences[s_idx], src_vocabulary_size)\n",
        "\n",
        "# Target sequences\n",
        "for t_idx in range(len(tgt_sequences)):\n",
        "  tgt_sequences[t_idx] = add_start_end_tokens(tgt_sequences[t_idx], tgt_vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxRM2u-PRCpR",
        "outputId": "a93340b3-c54c-4d9b-a127-16d58b08acde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1559, 1, 67, 45, 314, 2, 1, 57, 115, 1560],\n",
              " [2038, 71, 94, 100, 207, 1, 69, 50, 650, 2039])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_sequences[10], tgt_sequences[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ialbl7aRTrxD",
        "outputId": "50fb9bcf-87eb-4596-b0c3-268a510072f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source (English):\n",
            "-----------------\n",
            "(10000, 41)\n",
            "[1559    1   67   45  314    2    1   57  115 1560    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "Target (Hindi):\n",
            "-----------------\n",
            "(10000, 40)\n",
            "[2038   71   94  100  207    1   69   50  650 2039    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Pad the sequences to src_max_sequence_length (source) & tgt_max_sequence_length (target)\n",
        "src_padded_sequences = pad_sequences(src_sequences, \n",
        "                                     maxlen=src_max_sequence_length, \n",
        "                                     padding='post',\n",
        "                                     truncating='post')\n",
        "print(\"Source (English):\")\n",
        "print(\"-\"*17)\n",
        "print(src_padded_sequences.shape)\n",
        "print(src_padded_sequences[10])\n",
        "\n",
        "print()\n",
        "\n",
        "tgt_padded_sequences = pad_sequences(tgt_sequences, \n",
        "                                     maxlen=tgt_max_sequence_length,\n",
        "                                     padding='post',\n",
        "                                     truncating='post')\n",
        "print(\"Target (Hindi):\")\n",
        "print(\"-\"*17)\n",
        "print(tgt_padded_sequences.shape)\n",
        "print(tgt_padded_sequences[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpLalL_uMVUk"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "D_MODEL = 512\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 \n",
        "h = 8 # h: attention layers or heads\n",
        "p_dropout = 0.1 # rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUUTkyse1Ks7"
      },
      "source": [
        "**Positional Encoding [3]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DID5wRZk1JLE"
      },
      "outputs": [],
      "source": [
        "# The maximum length of sequence is 41 (max(src_max_sequence_length, tgt_max_sequence_length))\n",
        "# In this case, the positional encoding is computed for 100 positions\n",
        "p_encoding = np.ndarray((1, 100, D_MODEL)) # shape: 1x100x512 (100 positions x D_MODEL dimensions)\n",
        "\n",
        "for pos in range(100):\n",
        "    p_vector = []\n",
        "    for idx in range(D_MODEL):\n",
        "        x = pos / 10000 ** (2 * idx / D_MODEL)\n",
        "        if idx % 2 == 0: \n",
        "            p_vector.append(np.sin(x))  # even positions\n",
        "        else:\n",
        "            p_vector.append(np.cos(x))  # odd positions\n",
        "    p_encoding[0][pos] = np.array(p_vector)\n",
        "\n",
        "p_encoding = tf.convert_to_tensor(p_encoding, dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "OOoCg08WyVta",
        "outputId": "5720b5bd-8e35-45b0-dde3-9cb72aef3e54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f07322d9640>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot positional encodings\n",
        "plt.pcolormesh(p_encoding[0])\n",
        "plt.xlabel('idx')\n",
        "plt.ylabel('position')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3gVs53WKOZi"
      },
      "outputs": [],
      "source": [
        "class TransformerInput(Layer):\n",
        "  def __init__(self, vocabulary_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = Embedding(vocabulary_size, D_MODEL) \n",
        "\n",
        "    # Dropout layer\n",
        "    self.dropout = Dropout(p_dropout)\n",
        "    \n",
        "  def call(self, padded_sequences, training):\n",
        "    # Input (padded_sequences): BATCH_SIZE x sequence length\n",
        "    # 64x39 - Source (English) & Target (Hindi)\n",
        "    # Output (t_input): BATCH_SIZE x sequence length x D_MODEL \n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    t_input = self.embedding(padded_sequences)\n",
        "\n",
        "    # Add positional encoding to embeddings\n",
        "    # Positional encoding (p_encoding): 1x39x512 - Source (English) & Target (Hindi)\n",
        "    sequence_length = padded_sequences.shape[1] # Positions -> 0 to sequence_length - 1\n",
        "    t_input = tf.add(t_input, p_encoding[:, :sequence_length, :])\n",
        "\n",
        "    # Apply dropout\n",
        "    t_input = self.dropout(t_input, training)\n",
        "\n",
        "    # Output (t_input): BATCH_SIZE x sequence length x D_MODEL \n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    return t_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO1PsNrB5ghM"
      },
      "outputs": [],
      "source": [
        "class LinearLayer(Layer):\n",
        "  def __init__(self, d_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # Input to each encoder layer passes through Dense layers to generate Query (Q), Key (K) & Value (V) tensors \n",
        "    # Each Dense layer has (d_k/d_v)xh number of linear (f(x)=x) activation units\n",
        "    # because it's more efficient than h Dense layers in a loop with d_k/d_v linear activation units in each\n",
        "    # h (attention layers or heads) = 8\n",
        "    # d_k/d_v = 64\n",
        "    self.d_k = self.d_v = d_k\n",
        "\n",
        "    self.q_linear = Dense(D_MODEL)  # units = D_MODEL: 64x8 (d_kxh)\n",
        "    self.k_linear = Dense(D_MODEL)  # units = D_MODEL: 64x8 (d_kxh)\n",
        "    self.v_linear = Dense(D_MODEL)  # units = D_MODEL: 64x8 (d_vxh)\n",
        "\n",
        "  def call(self, q_input, k_input, v_input):\n",
        "    # Input (q_input, k_input, v_input): BATCH_SIZE x sequence length x D_MODEL \n",
        "    # Output (q, k, v): BATCH_SIZE x sequence length x D_MODEL \n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    q = self.q_linear(q_input) \n",
        "    k = self.k_linear(k_input)\n",
        "    v = self.v_linear(v_input)\n",
        "\n",
        "    # Reshape q, k, v from (BATCH_SIZE x sequence length x D_MODEL) to (BATCH_SIZE x h x sequence length x d_k/d_v)\n",
        "    # 64x8x39x64 - Source (English) & Target (Hindi)\n",
        "    # -1 for sequence length (39 - Source (English) & Target (Hindi))\n",
        "    q = tf.transpose(tf.reshape(q, shape=(BATCH_SIZE, -1, h, self.d_k)), perm=(0, 2, 1, 3))\n",
        "    k = tf.transpose(tf.reshape(k, shape=(BATCH_SIZE, -1, h, self.d_k)), perm=(0, 2, 1, 3)) \n",
        "    v = tf.transpose(tf.reshape(v, shape=(BATCH_SIZE, -1, h, self.d_v)), perm=(0, 2, 1, 3)) \n",
        "\n",
        "    return q, k, v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV6Wd9IxCJAH"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(Layer):\n",
        "  def __init__(self, d_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_k/d_v = 64\n",
        "    self.d_k = d_k\n",
        "    \n",
        "  def call(self, q, k, v, mask):\n",
        "    # Input (q, k): BATCH_SIZE x h x sequence length x d_k/d_v \n",
        "    # 64x8x39x64 - Source (English) & Target (Hindi)\n",
        "    # Output (attention_weights): BATCH_SIZE x h x sequence length x sequence length\n",
        "    # Source (English) & Target (Hindi): q.k_T = (64x8x39x64).(64x8x64x39) = (64x8x39x39)\n",
        "    attention_weights = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # Scale attention_weights by sqrt(d_k)\n",
        "    scaled_attention_weights = tf.divide(attention_weights, \n",
        "                                         tf.sqrt(tf.cast(self.d_k, dtype='float32')))\n",
        "\n",
        "    if mask is not None:\n",
        "      # Input, Output (mask): \n",
        "      # padding_mask - BATCH_SIZE x 1 x 1 x sequence length \n",
        "      # 64x1x1x39 - Source (English)\n",
        "      # look_ahead_mask - 1 x 1 x sequence length x sequence length\n",
        "      # 1x1x39x39 - Target (Hindi)\n",
        "      mask = mask * -1e25\n",
        "      scaled_attention_weights = tf.add(scaled_attention_weights, mask)\n",
        "\n",
        "    # Apply softmax on attention_weights\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_weights)\n",
        "\n",
        "    # Input (attention_weights, v): BATCH_SIZE x h x sequence length x sequence length\n",
        "    # 64x8x39x39 - Source (English) & Target (Hindi)\n",
        "    # Output (attention_output): \n",
        "    # Source (English) & Target (Hindi): attention_weights.v = (64x8x39x39).(64x8x39x64) = (64x8x39x64)\n",
        "    attention_output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return attention_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OGU2LDXCIq0"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self, d_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_k/d_v = 64\n",
        "    self.d_k = d_k\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    self.sdpa = ScaledDotProductAttention(self.d_k)\n",
        "\n",
        "    # The outputs of all attention heads are concatenated and passed through a Dense layer \n",
        "    # The Dense layer has D_MODEL number of linear (f(x)=x) activation units\n",
        "    self.z_linear = Dense(D_MODEL)\n",
        "\n",
        "  def call(self, q, k, v, mask=None):\n",
        "    # Input (q, k, v): BATCH_SIZE x h x sequence length x d_k/d_v\n",
        "    # Output (attention_output): BATCH_SIZE x h x sequence length x d_k/d_v\n",
        "    # 64x8x39x64 - Source (English) & Target (Hindi)\n",
        "    attention_output = self.sdpa(q, k, v, mask)\n",
        "\n",
        "    # Reshape attention_output from BATCH_SIZE x h x sequence length x d_k/d_v to BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    attention_output = tf.reshape(tf.transpose(attention_output, perm=(0, 2, 1, 3)), shape=(BATCH_SIZE, -1, D_MODEL))\n",
        "\n",
        "    # Output (mha_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    mha_output = self.z_linear(attention_output)\n",
        "\n",
        "    return mha_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvTgyjbywbMB"
      },
      "outputs": [],
      "source": [
        "class AddNorm(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Layer normalization of x + sublayer(x)\n",
        "    self.l_normalization = LayerNormalization()\n",
        "\n",
        "  def call(self, x, sublayer_x):\n",
        "    # Input (x, sublayer_x): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # Output(_sum): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    _sum = tf.add(x, sublayer_x)\n",
        "\n",
        "    # Output(an_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    an_output = self.l_normalization(_sum)\n",
        "\n",
        "    return(an_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWJ72Ev41HxC"
      },
      "outputs": [],
      "source": [
        "class FeedForward(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # The first Dense layer has 2048 'relu' activation units\n",
        "    self.d_ff = 2048\n",
        "    self.ff_1 = Dense(self.d_ff, activation='relu')\n",
        "\n",
        "    # The second Dense layer has D_MODEL linear activation units\n",
        "    self.ff_2 = Dense(D_MODEL)\n",
        "\n",
        "  def call(self, ff_input):\n",
        "    # Input (ff_input): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    # Output (ff_1_output): BATCH_SIZE x sequence length x d_ff\n",
        "    # 64x39x2048 - Source (English) & Target (Hindi)\n",
        "    ff_1_output = self.ff_1(ff_input)\n",
        "\n",
        "    # Output (ff_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Source (English) & Target (Hindi)\n",
        "    ff_output = self.ff_2(ff_1_output)\n",
        "\n",
        "    return ff_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xA9ZicdYqqb"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(Layer):\n",
        "  def __init__(self, d_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_k/d_v = 64\n",
        "    self.d_k = d_k\n",
        "\n",
        "    # Linear layer\n",
        "    self.l_layer = LinearLayer(self.d_k)\n",
        "\n",
        "    # Multi-head attention\n",
        "    self.mha = MultiHeadAttention(self.d_k)\n",
        "\n",
        "    # Dropout layers\n",
        "    self.dropout_1 = Dropout(p_dropout)\n",
        "    self.dropout_2 = Dropout(p_dropout)\n",
        "\n",
        "    # Add x & sublayer(x). Apply layer normalization on the sum\n",
        "    self.an_1 = AddNorm()\n",
        "    self.an_2 = AddNorm()\n",
        "\n",
        "    # Feed forward layer\n",
        "    self.ff = FeedForward()\n",
        "\n",
        "  def call(self, encoder_input, padding_mask, training):\n",
        "    # Input (encoder_input): BATCH_SIZE x sequence length x D_MODEL \n",
        "    # 64x39x512 - English\n",
        "    # Output (q, k, v): BATCH_SIZE x h x sequence length x d_k/d_v \n",
        "    # 64x8x39x64 - English\n",
        "    q, k, v = self.l_layer(encoder_input, encoder_input, encoder_input)\n",
        "      \n",
        "    # Output (mha_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - English\n",
        "    mha_output = self.mha(q, k, v, padding_mask)\n",
        "\n",
        "    # Apply dropout\n",
        "    mha_output = self.dropout_1(mha_output, training)\n",
        "\n",
        "    # Add encoder_input & mha_output. Apply layer normalization on the sum\n",
        "    # Output (ff_input): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - English\n",
        "    ff_input = self.an_1(encoder_input, mha_output)\n",
        "\n",
        "    # Output (ff_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - English\n",
        "    ff_output = self.ff(ff_input)\n",
        "\n",
        "    # Apply dropout\n",
        "    ff_output = self.dropout_2(ff_output, training)\n",
        "\n",
        "    # Add ff_input & ff_output. Apply layer normalization on the sum\n",
        "    # Output (encoder_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - English\n",
        "    encoder_output = self.an_2(ff_input, ff_output)\n",
        "    return encoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inu5m7uscrmg"
      },
      "outputs": [],
      "source": [
        "class Encoder(Layer):\n",
        "  def __init__(self, d_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_k/d_v = 64\n",
        "    self.d_k = d_k\n",
        "\n",
        "    # Input to first encoder layer\n",
        "    # +3 for [START]: src_vocabulary_size + 1 & [END]: src_vocabulary_size + 2\n",
        "    self.src_vocabulary_size = src_vocabulary_size + 3\n",
        "    self.t_input = TransformerInput(self.src_vocabulary_size)\n",
        "\n",
        "    # There are 6 encoder layers\n",
        "    self.n_encoder_layers = 6\n",
        "    self.encoder_layers = [EncoderLayer(self.d_k) for e in range(self.n_encoder_layers)]\n",
        "\n",
        "  def call(self, src_padded_sequences, src_padding_mask, training):\n",
        "    # Input (src_padded_sequences): BATCH_SIZE x sequence length \n",
        "    # 64x39 - English\n",
        "    # Output (encoder_input): BATCH_SIZE x sequence length x D_MODEL \n",
        "    # 64x39x512 - English\n",
        "    encoder_input = self.t_input(src_padded_sequences, training)\n",
        "\n",
        "    # Encoder layers\n",
        "    for e in range(self.n_encoder_layers):\n",
        "      # Output (encoder_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "      # 64x39x512 - English\n",
        "      encoder_output = self.encoder_layers[e](encoder_input, src_padding_mask, training)\n",
        "\n",
        "      # Output of encoder, n-1 is input to encoder, n\n",
        "      encoder_input = encoder_output\n",
        "    \n",
        "    return encoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4mzpcTJcha6"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(Layer):\n",
        "  def __init__(self, d_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_k/d_v = 64\n",
        "    self.d_k = d_k\n",
        "\n",
        "    # Linear layer\n",
        "    self.l_layer = LinearLayer(self.d_k)\n",
        "\n",
        "    # Masked multi-head attention & multi-head attention\n",
        "    self.m_mha = MultiHeadAttention(self.d_k)\n",
        "    self.mha = MultiHeadAttention(self.d_k)\n",
        "\n",
        "    # Dropout layers\n",
        "    self.dropout_1 = Dropout(p_dropout)\n",
        "    self.dropout_2 = Dropout(p_dropout)\n",
        "    self.dropout_3 = Dropout(p_dropout)\n",
        "\n",
        "    # Add x & sublayer(x). Apply layer normalization on the sum\n",
        "    self.an_1 = AddNorm()\n",
        "    self.an_2 = AddNorm()\n",
        "    self.an_3 = AddNorm()\n",
        "\n",
        "    # Feed forward layer\n",
        "    self.ff = FeedForward()\n",
        "\n",
        "  def call(self, encoder_output, decoder_input, padding_mask, look_ahead_mask, training):\n",
        "    # Output (m_q, m_k, m_v): BATCH_SIZE x h x sequence length x d_k/d_v \n",
        "    # 64x8x39x64 - Hindi\n",
        "    m_q, m_k, m_v = self.l_layer(decoder_input, decoder_input, decoder_input)\n",
        "      \n",
        "    # Output (m_mha_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    m_mha_output = self.m_mha(m_q, m_k, m_v, look_ahead_mask)\n",
        "\n",
        "    # Apply dropout\n",
        "    m_mha_output = self.dropout_1(m_mha_output, training)\n",
        "\n",
        "    # Add decoder_input & m_mha_output. Apply layer normalization on the sum\n",
        "    # Output (mha_input): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    mha_input = self.an_1(decoder_input, m_mha_output)\n",
        "      \n",
        "    # Input (q = mha_input, k = v = encoder_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    # Output (q, k, v): BATCH_SIZE x h x sequence length x d_k/d_v \n",
        "    # 64x8x39x64 - Hindi\n",
        "    q, k, v = self.l_layer(mha_input, encoder_output, encoder_output)\n",
        "      \n",
        "    # Output (mha_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    mha_output = self.mha(q, k, v, padding_mask)\n",
        "\n",
        "    # Apply dropout\n",
        "    mha_output = self.dropout_2(mha_output, training)\n",
        "    \n",
        "    # Add mha_input & mha_output. Apply layer normalization on the sum\n",
        "    # Output (ff_input): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    ff_input = self.an_2(mha_input, mha_output)\n",
        "\n",
        "    # Output (ff_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    ff_output = self.ff(ff_input)\n",
        "\n",
        "    # Apply dropout\n",
        "    ff_output = self.dropout_3(ff_output, training)\n",
        "\n",
        "    # Add ff_input & ff_output. Apply layer normalization on the sum\n",
        "    # Output (decoder_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    decoder_output = self.an_3(ff_input, ff_output)\n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsgNYXzWdm3H"
      },
      "outputs": [],
      "source": [
        "class Decoder(Layer):\n",
        "  def __init__(self, d_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_k/d_v = 64\n",
        "    self.d_k = d_k\n",
        "\n",
        "    # Input to first decoder layer\n",
        "    # +3 for [START]: tgt_vocabulary_size + 1 & [END]: tgt_vocabulary_size + 2\n",
        "    self.tgt_vocabulary_size = tgt_vocabulary_size + 3\n",
        "    self.t_input = TransformerInput(self.tgt_vocabulary_size)\n",
        "\n",
        "    # There are 6 decoder layers\n",
        "    self.n_decoder_layers = 6\n",
        "    self.decoder_layers = [DecoderLayer(self.d_k) for d in range(self.n_decoder_layers)]\n",
        "\n",
        "  def call(self, tgt_padded_sequences, encoder_output, src_padding_mask, look_ahead_mask, training):\n",
        "    # Input (tgt_padded_sequences): BATCH_SIZE x sequence length \n",
        "    # 64x39 - Hindi\n",
        "    # Output (decoder_input): BATCH_SIZE x sequence length x D_MODEL \n",
        "    # 64x39x512 - Hindi\n",
        "    decoder_input = self.t_input(tgt_padded_sequences, training)\n",
        "\n",
        "    # Decoder layers\n",
        "    for d in range(self.n_decoder_layers):\n",
        "      # Output (decoder_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "      # 64x39x512 - Hindi\n",
        "      decoder_output = self.decoder_layers[d](encoder_output, decoder_input, src_padding_mask, look_ahead_mask, training)\n",
        "\n",
        "      # Output of decoder, n-1 is input to decoder, n\n",
        "      decoder_input = decoder_output\n",
        "    \n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8RWt6CVqjJp"
      },
      "source": [
        "**Masking [4]:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Why is padding mask applied only along the key axis (columns)? \n",
        "\n",
        "    It is to remove the impact of padded tokens on other tokens in the sequence\n",
        "\n",
        "\n",
        "*   Why is padding mask not applied along the query axis (rows)? \n",
        "    \n",
        "    It is to retain information about the padded tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIq9OdcAArTN"
      },
      "outputs": [],
      "source": [
        "class Transformer(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # d_k/d_v = D_MODEL/h = 512/8 = 64\n",
        "    self.d_k = self.d_v = D_MODEL // h\n",
        "\n",
        "    # Encoder\n",
        "    self.encoder = Encoder(self.d_k)\n",
        "\n",
        "    # Decoder\n",
        "    self.decoder = Decoder(self.d_k)\n",
        "\n",
        "    # Linear layer for the final output\n",
        "    # +3 for [START]: tgt_vocabulary_size + 1 & [END]: tgt_vocabulary_size + 2\n",
        "    self.tgt_vocabulary_size = tgt_vocabulary_size + 3\n",
        "    self.o_linear = Dense(self.tgt_vocabulary_size)\n",
        "\n",
        "  def compute_padding_mask(self, padded_sequences):\n",
        "    # Input (padded_sequences), Output (mask): BATCH_SIZE x sequence length \n",
        "    # 64x39 - Source (English) & Target (Hindi)\n",
        "    # Mask value = 1 for padding (padding value=0)\n",
        "    # Mask value = 0 for all the other tokens\n",
        "    mask = tf.where(tf.equal(padded_sequences, 0), \n",
        "                    tf.ones_like(padded_sequences),\n",
        "                    tf.zeros_like(padded_sequences)) \n",
        "    \n",
        "    # Output (mask): BATCH_SIZE x 1 x 1 x sequence length \n",
        "    # 64x1x1x39 - Source (English) & Target (Hindi)\n",
        "    mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype='float32')\n",
        "    return mask\n",
        "\n",
        "  def compute_look_ahead_mask(self, tgt_padding_mask):\n",
        "    # Output (look_ahead_mask): 1 x 1 x sequence length x sequence length\n",
        "    # 1x1x39x39 - Target (Hindi)\n",
        "    sequence_length = tgt_padding_mask.shape[-1]\n",
        "    look_ahead_mask = np.triu(np.ones((1, 1, sequence_length, sequence_length)), k=1) \n",
        "    look_ahead_mask = tf.convert_to_tensor(look_ahead_mask, dtype='float32')\n",
        "    look_ahead_mask = tf.maximum(tgt_padding_mask, look_ahead_mask)\n",
        "    return look_ahead_mask\n",
        "\n",
        "  def call(self, src_padded_sequences, tgt_padded_sequences, training=True):\n",
        "    # Input (src_padded_sequences): BATCH_SIZE x sequence length \n",
        "    # 64x39 - English\n",
        "    # Output (src_padding_mask): BATCH_SIZE x 1 x 1 x sequence length \n",
        "    # 64x1x1x39 - English\n",
        "    src_padding_mask = self.compute_padding_mask(src_padded_sequences)\n",
        "    \n",
        "    # Output (encoder_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - English\n",
        "    encoder_output = self.encoder(src_padded_sequences, src_padding_mask, training)\n",
        "\n",
        "    # Input (tgt_padded_sequences): BATCH_SIZE x sequence length \n",
        "    # 64x39 - Hindi\n",
        "    # Output (tgt_padding_mask): BATCH_SIZE x 1 x 1 x sequence length \n",
        "    # 64x1x1x39 - Hindi\n",
        "    tgt_padding_mask = self.compute_padding_mask(tgt_padded_sequences)\n",
        "\n",
        "    # Output (look_ahead_mask): 1 x 1 x sequence length x sequence length\n",
        "    # 1x1x39x39 - Target (Hindi)\n",
        "    look_ahead_mask = self.compute_look_ahead_mask(tgt_padding_mask)\n",
        "\n",
        "    # Output (decoder_output): BATCH_SIZE x sequence length x D_MODEL\n",
        "    # 64x39x512 - Hindi\n",
        "    decoder_output = self.decoder(tgt_padded_sequences, encoder_output, src_padding_mask, look_ahead_mask, training)\n",
        "\n",
        "    # Output (t_output): BATCH_SIZE x sequence length x target vocabulary\n",
        "    # 64x39x2040 - Hindi (target vocabulary + 3 = 2037 + 3 = 2040)\n",
        "    t_output = self.o_linear(decoder_output)\n",
        "    return t_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQBU_WPnn8Vm"
      },
      "outputs": [],
      "source": [
        "class LRSchedule(LearningRateSchedule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Warmup steps = 4000\n",
        "    self.warmup_steps = 4000\n",
        "\n",
        "  def __call__(self, step_num):\n",
        "    # Compute learning rate as mentioned in the paper [1]\n",
        "    step_num = tf.cast(step_num, dtype='float64')\n",
        "    l_rate = (D_MODEL ** -.5) * tf.minimum((step_num ** -.5),\n",
        "                                           step_num * (self.warmup_steps ** -1.5))\n",
        "    return l_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXXvkbbzi2bD"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "transformer = Transformer()\n",
        "\n",
        "# Optimizer with custom learning rate\n",
        "lr_schedule = LRSchedule()\n",
        "optimizer = Adam(learning_rate=lr_schedule, beta_1=.9, beta_2=.98, epsilon=1e-9)\n",
        "\n",
        "# Loss\n",
        "# reduction is set to 'none' to ignore padding during loss computation\n",
        "# If padding is not ignored during loss computation, training is impacted\n",
        "scce = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Metric\n",
        "train_loss = Mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0tTB2b8ATrM"
      },
      "outputs": [],
      "source": [
        "# Split source, target padded sequences to train (80%) and test (20%)\n",
        "train_src_padded_sequences, train_tgt_padded_sequences = src_padded_sequences[:8000, :39], tgt_padded_sequences[:8000, :40]\n",
        "test_src_padded_sequences, test_tgt_padded_sequences = src_padded_sequences[8000:, :39], tgt_padded_sequences[8000:, :40]\n",
        "\n",
        "# Shuffle and create batches\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_src_padded_sequences, train_tgt_padded_sequences))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_src_padded_sequences, test_tgt_padded_sequences))\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTcBCPDdasB9"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "EPOCHS = 250\n",
        "\n",
        "# Checkpoint\n",
        "ckpt = tf.train.Checkpoint(optimizer=optimizer, transformer=transformer)\n",
        "ckpt_mngr = tf.train.CheckpointManager(ckpt, './t_checkpoints', max_to_keep=3)\n",
        "\n",
        "def compute_loss(actual_y, predicted_y):\n",
        "  # Padding mask: 0 for padding, 1 for rest\n",
        "  padding_mask = tf.where(tf.equal(actual_y, 0),\n",
        "                          tf.zeros_like(actual_y),\n",
        "                          tf.ones_like(actual_y))\n",
        "  padding_mask = tf.cast(padding_mask, dtype='float32')\n",
        "\n",
        "  # Loss (raw tensor)\n",
        "  loss = scce(actual_y, predicted_y)\n",
        "  loss = tf.cast(loss, dtype='float32')\n",
        "\n",
        "  # Multiply (element-wise) loss with padding mask \n",
        "  # The loss for padding becomes 0\n",
        "  loss *= padding_mask\n",
        "\n",
        "  # Compute mean loss\n",
        "  loss = tf.reduce_sum(loss) / tf.reduce_sum(padding_mask)\n",
        "  return loss\n",
        "\n",
        "def train_transformer(src_padded_sequences, tgt_padded_sequences):\n",
        "  # Shift tgt_padded_sequences to right by 1 position\n",
        "  actual_t_output = tgt_padded_sequences[:, 1:]\n",
        "\n",
        "  # Now, input to decoder is tgt_padded_sequences[, :-1]\n",
        "  decoder_input = tgt_padded_sequences[:, :-1]\n",
        "  \n",
        "  with tf.GradientTape() as gt:\n",
        "    # Forward pass\n",
        "    predicted_t_output = transformer(src_padded_sequences, decoder_input)\n",
        "\n",
        "    # Compute loss, L\n",
        "    loss = compute_loss(actual_t_output, predicted_t_output)\n",
        "  \n",
        "  # grad(L) w.r.t all transformer weights\n",
        "  t_weights = transformer.trainable_variables\n",
        "  t_gradients = gt.gradient(loss, t_weights)  \n",
        "\n",
        "  # Update the weights\n",
        "  optimizer.apply_gradients(zip(t_gradients, t_weights))\n",
        "  train_loss(loss)\n",
        "\n",
        "# Restore latest checkpoint, if any\n",
        "ckpt.restore(ckpt_mngr.latest_checkpoint)\n",
        "if not ckpt_mngr.latest_checkpoint:\n",
        "  print(\"Nothing to restore, initializing from the beginning...\")\n",
        "else:\n",
        "  print(\"Restoring latest checkpoint: {}\".format(ckpt_mngr.latest_checkpoint))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  for (batch, (src_padded_sequences, tgt_padded_sequences)) in enumerate(train_dataset):\n",
        "    train_transformer(src_padded_sequences, tgt_padded_sequences)\n",
        "  \n",
        "  end = time.time()\n",
        "\n",
        "  # Save checkpoint\n",
        "  ckpt_path = ckpt_mngr.save()\n",
        "  print(\"Saving checkpoint {} for epoch {}\".format(ckpt_path, epoch+1))\n",
        "\n",
        "  print(\"Epoch {}/{} ==========> - {:.5f}s - loss: {:.5f}\".format(epoch+1, EPOCHS, end-start, train_loss.result()))\n",
        "  train_loss.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUCywHQHZdRt",
        "outputId": "85968e2b-5d7f-4665-e35f-297a315f2a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Source:\n",
            "-------\n",
            "['leak resolution']\n",
            "\n",
            "Target (Expected):\n",
            "------------------\n",
            "['लीक विभेदनः']\n",
            "\n",
            "Target (Predicted):\n",
            "-------------------\n",
            "['लीक विभेदनः']\n",
            "\n",
            "\n",
            "Source:\n",
            "-------\n",
            "['devhelp plugin for anjuta']\n",
            "\n",
            "Target (Expected):\n",
            "------------------\n",
            "['अंजुटा के लिए डेव वहेल्प प्लगिन']\n",
            "\n",
            "Target (Predicted):\n",
            "-------------------\n",
            "['अंजुटा के लिए डेव वहेल्प प्लगिन']\n",
            "\n",
            "\n",
            "Source:\n",
            "-------\n",
            "['sample file operations']\n",
            "\n",
            "Target (Expected):\n",
            "------------------\n",
            "['नमूना फाइल आपरेशन']\n",
            "\n",
            "Target (Predicted):\n",
            "-------------------\n",
            "['डिस्क के उन्नत विकल्पों को छुपाओ']\n",
            "\n",
            "\n",
            "Source:\n",
            "-------\n",
            "['select program to run']\n",
            "\n",
            "Target (Expected):\n",
            "------------------\n",
            "['चलाने के लिए प्रोग्राम चुनेंः']\n",
            "\n",
            "Target (Predicted):\n",
            "-------------------\n",
            "['चलाने के लिए प्रोग्राम चुनेंः']\n",
            "\n",
            "\n",
            "Source:\n",
            "-------\n",
            "['toggle fullscreen mode']\n",
            "\n",
            "Target (Expected):\n",
            "------------------\n",
            "['पूरा स्क्रीन मोड टॉगल करें']\n",
            "\n",
            "Target (Predicted):\n",
            "-------------------\n",
            "['पूरा स्क्रीन मोड टॉगल करें']\n",
            "\n",
            "\n",
            "Source:\n",
            "-------\n",
            "['disable syntax highlighting']\n",
            "\n",
            "Target (Expected):\n",
            "------------------\n",
            "['वाक्य रचना हाइलाइटिंग निष्क्रिय करें']\n",
            "\n",
            "Target (Predicted):\n",
            "-------------------\n",
            "['वाक्य रचना हाइलाइटिंग निष्क्रिय करें']\n",
            "\n",
            "Source:\n",
            "-------\n",
            "['save file as']\n",
            "\n",
            "Target (Expected):\n",
            "------------------\n",
            "['ऐसे फाइल सहेजें']\n",
            "\n",
            "Target (Predicted):\n",
            "-------------------\n",
            "['ऐसे सहेजें को उभारें है']\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "for idx in np.random.randint(2000, size=7):\n",
        "    # Source\n",
        "    src = test_src_padded_sequences[idx]\n",
        "    src = tf.expand_dims(src, 0)\n",
        "    print(\"\\n\\nSource:\")\n",
        "    print(\"-\"*7)\n",
        "    print(src_tknzr.sequences_to_texts(src[:, 1:-1].numpy()))\n",
        "\n",
        "    # Target\n",
        "    tgt = test_tgt_padded_sequences[idx]\n",
        "    tgt = tf.expand_dims(tgt, 0)\n",
        "    print(\"\\nTarget (Expected):\")\n",
        "    print(\"-\"*18)\n",
        "    print(tgt_tknzr.sequences_to_texts(tgt[:, 1:-1].numpy()))\n",
        "\n",
        "    # Initial decoder input is [START]\n",
        "    decoder_input = test_tgt_padded_sequences[idx][:1]\n",
        "    decoder_input = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    # tgt_max_sequence_length = 40\n",
        "    for i in range(tgt_max_sequence_length):\n",
        "        # Get the next prediction (word) in target (Hindi) language \n",
        "        predicted_t_output = transformer(src, decoder_input, training=False)\n",
        "        predicted_t_output = predicted_t_output[: ,-1:, :]\n",
        "        predicted_token = tf.cast(tf.argmax(predicted_t_output, axis=-1), tf.int32)\n",
        "        # Concatenate output from time steps, 1 to (t-1) and output from time step, t\n",
        "        decoder_input = tf.concat([decoder_input, predicted_token], axis=-1)\n",
        "        # Stop if the prediction is [END]\n",
        "        if predicted_token == (tgt_vocabulary_size + 2):\n",
        "            break\n",
        "\n",
        "    print(\"\\nTarget (Predicted):\")\n",
        "    print(\"-\"*19)\n",
        "    print(tgt_tknzr.sequences_to_texts(decoder_input[:, 1:-1].numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24NuZWF6mT-y"
      },
      "source": [
        "**References:**\n",
        "\n",
        "\n",
        "1.   Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems (2017).\n",
        "2.   Yannic Kilcher (Youtube) – Attention is All You Need\n",
        "3.   Hedu AI (Youtube) – Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings\n",
        "4.   https://medium.com/mlearning-ai/how-do-self-attention-masks-work-72ed9382510f\n",
        "5.   https://www.tensorflow.org/text/tutorials/transformer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}